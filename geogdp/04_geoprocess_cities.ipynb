{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import glob\n",
    "from urllib import urlretrieve\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import mapping, shape\n",
    "from osgeo import gdal, gdalnumeric, ogr, osr\n",
    "from gdalconst import *\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from library.geoprocess import *\n",
    "from library.cdb_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create individual shapefiles of each country from shapefile of all countries\n",
    "* load shapefile of all admin areas / countries as geodataframe\n",
    "* filter out countries not internationally recognized\n",
    "* loop through rows of geodataframe and save each row as a country-specific shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peter/anaconda/lib/python2.7/site-packages/geopandas/geodataframe.py:376: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  result = super(GeoDataFrame, self).__getitem__(key)\n"
     ]
    }
   ],
   "source": [
    "# load shapefile of all admin areas / countries as geodataframe\n",
    "gdf = gpd.read_file('data/geo/countries/countries_nf2.shp'); gdf.head(3)\n",
    "\n",
    "# filter out countries not internationally recognized\n",
    "country_filter1 = gdf['WB_A3'] != '-99'\n",
    "gdf = gdf.drop_duplicates(subset='WB_A3')\n",
    "gdf = gdf[country_filter1].set_index('WB_A3')\n",
    "\n",
    "# loop through rows of geodataframe and save each row as a country-specific shapefile in newly created dir\n",
    "# shp_to_shps('data/geo/countries/shp', gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate city boundaries\n",
    "* Clip master raster from 2013 by each country shapefile, creating country-specific rasters\n",
    "* Use subprocess module to run gdal commands in terminal to do this\n",
    "* Polygonize each country raster\n",
    "* Select subset of polygons that have light intensity greater than selected thresh\n",
    "* Union remaining polygons to get contiguous city boundaries\n",
    "* Intersect with populated places to eliminate non-key cities\n",
    "* Save outputs to cities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clip master raster from 2013 by each country shapefile to create country-level rasters\n",
    "input_tif_path = 'data/geo/images/F182013.v4c_web.stable_lights.avg_vis.tif'\n",
    "input_shp_dir = 'data/geo/countries/shp'\n",
    "output_tif_dir = 'data/geo/countries/tif'\n",
    "countries = [x.encode('UTF-8') for x in gdf.index.values]\n",
    "# raster_to_rasters(countries, input_tif_path, input_shp_dir, output_tif_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# polygonize rasters and save to target directory\n",
    "input_tif_dir = 'data/geo/countries/tif'\n",
    "output_shp_dir = 'data/geo/countries/poly'\n",
    "# polygonize(input_tif_dir, output_shp_dir, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter and union countries, save to target directory\n",
    "input_dir = 'data/geo/countries/poly'\n",
    "output_dir = 'data/geo/cities/union'\n",
    "# union_and_filter(input_dir, output_dir, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split multi-polygons into polygons\n",
    "input_dir = 'data/geo/cities/union'\n",
    "output_dir = 'data/geo/cities/split'\n",
    "# split_multi_to_single_poly(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge shapefiles in directory\n",
    "input_dir = 'data/geo/cities/split'\n",
    "output_dir = 'data/geo/cities/merge'\n",
    "output_filename = 'merged.shp'\n",
    "# merge_shapefiles(input_dir, output_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set CRS of merged shapefile\n",
    "input_path = 'data/geo/cities/merge/merged.shp'\n",
    "crs = 'epsg:4326'\n",
    "output_path = 'data/geo/cities/merge/merged_crs.shp'\n",
    "# set_crs(input_path, crs, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zip merged shapefiles\n",
    "target_dir = 'data/geo/cities/merge'\n",
    "shp_filename = 'merged_crs.shp'\n",
    "zip_filename = 'merged_crs.zip'\n",
    "\n",
    "shp_path = os.path.join(target_dir, shp_filename)\n",
    "zip_path = os.path.join(target_dir, zip_filename)\n",
    "zip_path = os.path.abspath(zip_path)\n",
    "shp_filename_no_ext = shp_filename[:-4]\n",
    "glob_string = os.path.join(target_dir, shp_filename_no_ext) + '*'\n",
    "list_of_shps = glob.glob(glob_string)\n",
    "list_of_shps = [os.path.abspath(x) for x in list_of_shps]\n",
    "\n",
    "#zip_files(list_of_shps, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdb_api_key = 'your_api_key'\n",
    "cdb_domain = 'your_username'\n",
    "c = CartoDBAPIKey(cdb_api_key, cdb_domain)\n",
    "url = 'http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places_simple.zip'\n",
    "\n",
    "# upload populated places shapefiles to cartodb\n",
    "fi_1 = URLImport(url, c, privacy='public')\n",
    "\n",
    "# upload zipped merged_crs shapefiles to cartodb\n",
    "file_to_import = 'data/geo/cities/merge/merged_crs.zip'\n",
    "fi_2 = FileImport(file_to_import, c, privacy='public')\n",
    "\n",
    "#fi_1.run()\n",
    "#fi_2.run()\n",
    "#fi_1.success, fi_2.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call cartodb sql api to get polygons that intersect with cites, format as geojson\n",
    "#intersection = c.sql(sql = 'with pop as (select nameascii, adm0_a3, pop_max, the_geom from ne_10m_populated_places_simple where pop_max > 1000000) select merged_crs.cartodb_id, pop.nameascii, pop.adm0_a3, pop.pop_max, merged_crs.the_geom from pop, merged_crs where ST_Within(pop.the_geom, merged_crs.the_geom)', format='geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump geojson output from cartodb into file\n",
    "dir_intersect = 'data/geo/cities/intersect'\n",
    "filename = 'cities.geojson'\n",
    "path = os.path.join(dir_intersect, filename)\n",
    "\n",
    "#rm_and_mkdir(dir_intersect)\n",
    "#with open(path, 'w') as outfile:\n",
    "    #json.dump(intersection, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write geojson to shapefile in same direcory: these are the metro clusters\n",
    "shp_path = 'data/geo/cities/intersect/cities.shp'\n",
    "geojson_path = 'data/geo/cities/intersect/cities.geojson'\n",
    "#subprocess.check_call(['ogr2ogr', '-F', 'ESRI Shapefile', shp_path, geojson_path, 'OGRGeoJSON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get zonal stats for each metro cluster\n",
    "* Load geodataframe of metro clusters\n",
    "* Run zonal stats function on each cluster across each image\n",
    "* Dump to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load cities shapefile and get zonal stats\n",
    "tif_dir = 'data/geo/images'\n",
    "input_shp_path = 'data/geo/cities/intersect/cities.shp'\n",
    "#gdf = zonal_to_shp(tif_dir, shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump to pickle\n",
    "#with open('data/geo/pickles/zonal_stats_m.pickle', 'wb') as f:\n",
    "    #pickle.dump(gdf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
